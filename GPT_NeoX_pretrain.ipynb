{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download git repository"
      ],
      "metadata": {
        "id": "9C6zDnix06sY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSSCnNL6OCHl",
        "outputId": "a63702ca-3b68-47c2-edb5-f4b341769e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-neox'...\n",
            "remote: Enumerating objects: 16065, done.\u001b[K\n",
            "remote: Counting objects: 100% (762/762), done.\u001b[K\n",
            "remote: Compressing objects: 100% (318/318), done.\u001b[K\n",
            "remote: Total 16065 (delta 513), reused 646 (delta 439), pack-reused 15303\u001b[K\n",
            "Receiving objects: 100% (16065/16065), 104.67 MiB | 34.40 MiB/s, done.\n",
            "Resolving deltas: 100% (11495/11495), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EleutherAI/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ],
      "metadata": {
        "id": "IyN0rqGQ1B2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpt-neox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThjL_A20Pb4C",
        "outputId": "ee6ceb0a-40e8-4b49-94e1-3badc89ec3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements/requirements.txt\n",
        "!pip install tensorboard==2.14\n",
        "!pip install 'urllib3<2'\n",
        "!pip install -r requirements/requirements-wandb.txt # optional, if logging using WandB\n",
        "!pip install -r requirements/requirements-tensorboard.txt # optional, if logging via tensorboard\n",
        "!python ./megatron/fused_kernels/setup.py install # optional, if using fused kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "reX0I26aPXvY",
        "outputId": "ecf0c409-20c4-4ad7-f26c-c6be26a5fc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed (from -r requirements/requirements.txt (line 2))\n",
            "  Cloning https://github.com/EleutherAI/DeeperSpeed.git to /tmp/pip-install-sud48h7a/deepspeed_ba21f307d52144919f15465e8b365de7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/DeeperSpeed.git /tmp/pip-install-sud48h7a/deepspeed_ba21f307d52144919f15465e8b365de7\n",
            "  Resolved https://github.com/EleutherAI/DeeperSpeed.git to commit a48c6493f1fc6a1652723ab5542f7703be6f03a5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/EleutherAI/lm_dataformat.git@4eec05349977071bf67fc072290b95e31c8dd836 (from -r requirements/requirements.txt (line 4))\n",
            "  Cloning https://github.com/EleutherAI/lm_dataformat.git (to revision 4eec05349977071bf67fc072290b95e31c8dd836) to /tmp/pip-req-build-b27smmqd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm_dataformat.git /tmp/pip-req-build-b27smmqd\n",
            "  Running command git rev-parse -q --verify 'sha^4eec05349977071bf67fc072290b95e31c8dd836'\n",
            "  Running command git fetch -q https://github.com/EleutherAI/lm_dataformat.git 4eec05349977071bf67fc072290b95e31c8dd836\n",
            "  Resolved https://github.com/EleutherAI/lm_dataformat.git to commit 4eec05349977071bf67fc072290b95e31c8dd836\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting best_download (from -r requirements/requirements.txt (line 1))\n",
            "  Downloading best_download-0.0.9-py3-none-any.whl (6.5 kB)\n",
            "Collecting ftfy>=6.0.1 (from -r requirements/requirements.txt (line 3))\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements/requirements.txt (line 5)) (0.19.4)\n",
            "Collecting lm_eval==0.3.0 (from -r requirements/requirements.txt (line 6))\n",
            "  Downloading lm_eval-0.3.0-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpi4py>=3.0.3 (from -r requirements/requirements.txt (line 7))\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements/requirements.txt (line 8)) (1.23.5)\n",
            "Collecting pybind11>=2.6.2 (from -r requirements/requirements.txt (line 9))\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements/requirements.txt (line 10)) (2023.6.3)\n",
            "Collecting sentencepiece (from -r requirements/requirements.txt (line 11))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from -r requirements/requirements.txt (line 12)) (1.16.0)\n",
            "Collecting tiktoken>=0.1.2 (from -r requirements/requirements.txt (line 13))\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements/requirements.txt (line 14)) (0.15.0)\n",
            "Collecting transformers==4.30.2 (from -r requirements/requirements.txt (line 15))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2.8.7)\n",
            "Collecting openai>=0.6.4 (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycountry (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytablewriter (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu==1.5.0 (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.2.2)\n",
            "Collecting sqlitedict (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2.1.0+cu118)\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Collecting zstandard (from lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (2.31.0)\n",
            "Collecting tokenizers>=0.12.1 (from -r requirements/requirements.txt (line 14))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements/requirements.txt (line 15)) (4.66.1)\n",
            "Collecting portalocker (from sacrebleu==1.5.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting rehash (from best_download->-r requirements/requirements.txt (line 1))\n",
            "  Downloading rehash-1.0.1-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting hjson (from deepspeed->-r requirements/requirements.txt (line 2))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed->-r requirements/requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements/requirements.txt (line 2)) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements/requirements.txt (line 2)) (1.10.13)\n",
            "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy>=6.0.1->-r requirements/requirements.txt (line 3))\n",
            "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
            "Collecting ujson (from lm-dataformat==0.0.20->-r requirements/requirements.txt (line 4))\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.11.0->-r requirements/requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.11.0->-r requirements/requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.8.6)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements/requirements.txt (line 15)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements/requirements.txt (line 15)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements/requirements.txt (line 15)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements/requirements.txt (line 15)) (2023.7.22)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (67.7.2)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
            "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Collecting colorama (from tqdm-multiprocess->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.3.1)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2023.3.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->lm_eval==0.3.0->-r requirements/requirements.txt (line 6)) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai>=0.6.4->lm_eval==0.3.0->-r requirements/requirements.txt (line 6))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed, lm-dataformat, mpi4py, rouge-score, pycountry, sqlitedict\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.9.3+a48c649-py3-none-any.whl size=834681 sha256=0afb20e7c524157e7142d1ecf72ef39a2f10fa7f53d1ccc924fb6414baf24750\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mfkz1pml/wheels/f6/a1/f3/0bf0723fbe58f0eef90d0196e5bcb3ad3a8e8b27f6bc47fdaa\n",
            "  Building wheel for lm-dataformat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm-dataformat: filename=lm_dataformat-0.0.20-py3-none-any.whl size=5836 sha256=77d4ed8859a5c0b2eb4ed77a75f734793cdacaed660dcee8559318c936585ab6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/5d/39/858622f394e968f5055c63d6023137b79341dfe415baf84098\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746496 sha256=05e31b74de3c337c1cd1360a93b97e15cb7888c0c03da16c18a9df0a8542512c\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=ef4909acd886f0edfdcff7d98fd9b18126fa1c45a59a81b50e79bfa5095ac0a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=cb23af2d60de52c7d5db0d42870b339ae180fa1aaf633cf0083dd23d00a909fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=7f32e10bc98dcea90ee72c71d5e664160ba1e901257833dd1f1ab32c996451d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built deepspeed lm-dataformat mpi4py rouge-score pycountry sqlitedict\n",
            "Installing collected packages: wcwidth, tokenizers, sqlitedict, sentencepiece, rehash, ninja, hjson, zstandard, ujson, tcolorpy, pycountry, pybind11, pyarrow-hotfix, portalocker, pathvalidate, mpi4py, mbstrdecoder, jsonlines, h11, ftfy, dill, colorama, typepy, tqdm-multiprocess, tiktoken, sacrebleu, rouge-score, multiprocess, lm-dataformat, httpcore, best_download, transformers, httpx, deepspeed, openai, datasets, DataProperty, tabledata, pytablewriter, lm_eval\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.10\n",
            "    Uninstalling wcwidth-0.2.10:\n",
            "      Successfully uninstalled wcwidth-0.2.10\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.0.1 best_download-0.0.9 colorama-0.4.6 datasets-2.15.0 deepspeed-0.9.3+a48c649 dill-0.3.7 ftfy-6.1.3 h11-0.14.0 hjson-3.1.0 httpcore-1.0.2 httpx-0.25.1 jsonlines-4.0.0 lm-dataformat-0.0.20 lm_eval-0.3.0 mbstrdecoder-1.1.3 mpi4py-3.1.5 multiprocess-0.70.15 ninja-1.11.1.1 openai-1.3.5 pathvalidate-3.2.0 portalocker-2.8.2 pyarrow-hotfix-0.6 pybind11-2.11.1 pycountry-22.3.5 pytablewriter-1.2.0 rehash-1.0.1 rouge-score-0.1.2 sacrebleu-1.5.0 sentencepiece-0.1.99 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 tiktoken-0.5.1 tokenizers-0.13.3 tqdm-multiprocess-0.0.11 transformers-4.30.2 typepy-1.3.2 ujson-5.8.0 wcwidth-0.2.12 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb>=0.10.28 (from -r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading sentry_sdk-1.36.0-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (6.0.1)\n",
            "Collecting setproctitle (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1)) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.28->-r requirements/requirements-wandb.txt (line 1))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.36.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
            "Collecting tensorboard==2.13.0 (from -r requirements/requirements-tensorboard.txt (line 1))\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.59.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (0.41.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements/requirements-tensorboard.txt (line 1)) (3.2.2)\n",
            "Installing collected packages: tensorboard\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.13.0\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating fused_kernels.egg-info\n",
            "writing fused_kernels.egg-info/PKG-INFO\n",
            "writing dependency_links to fused_kernels.egg-info/dependency_links.txt\n",
            "writing top-level names to fused_kernels.egg-info/top_level.txt\n",
            "writing manifest file 'fused_kernels.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fused_kernels.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'fused_kernels.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "creating /content/gpt-neox/build\n",
            "creating /content/gpt-neox/build/temp.linux-x86_64-cpython-310\n",
            "creating /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content\n",
            "creating /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox\n",
            "creating /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron\n",
            "creating /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels\n",
            "Emitting ninja build file /content/gpt-neox/build/temp.linux-x86_64-cpython-310/build.ninja...\n",
            "Compiling objects...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "[2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax.o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "building 'scaled_masked_softmax_cuda' extension\n",
            "Emitting ninja build file /content/gpt-neox/build/temp.linux-x86_64-cpython-310/build.ninja...\n",
            "Compiling objects...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.cpp -o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "[2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.h(343): warning #177-D: variable \"batch_count\" was declared but never referenced\n",
            "\n",
            "/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.h(343): warning #177-D: variable \"batch_count\" was declared but never referenced\n",
            "\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax.o /content/gpt-neox/build/temp.linux-x86_64-cpython-310/content/gpt-neox/megatron/fused_kernels/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for scaled_upper_triang_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "creating stub loader for scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/scaled_upper_triang_masked_softmax_cuda.py to scaled_upper_triang_masked_softmax_cuda.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/scaled_masked_softmax_cuda.py to scaled_masked_softmax_cuda.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fused_kernels.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fused_kernels.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fused_kernels.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fused_kernels.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.scaled_masked_softmax_cuda.cpython-310: module references __file__\n",
            "__pycache__.scaled_upper_triang_masked_softmax_cuda.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/fused_kernels-0.0.1-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing fused_kernels-0.0.1-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/fused_kernels-0.0.1-py3.10-linux-x86_64.egg\n",
            "Extracting fused_kernels-0.0.1-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding fused-kernels 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/fused_kernels-0.0.1-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for fused-kernels==0.0.1\n",
            "Finished processing dependencies for fused-kernels==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets And Tokenizer"
      ],
      "metadata": {
        "id": "aRMNFIl51H_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "U1idnscmPaEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Custom jsonl Dataset(Optional)"
      ],
      "metadata": {
        "id": "73LhqWHm1UFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "my_dataset = open(\"my_dataset.txt\", 'r').read()\n",
        "my_dataset = my_dataset.split(\"\\n\")\n",
        "\n",
        "# make data into list\n",
        "list_dict = []\n",
        "for i in range(len(my_dataset)):\n",
        "    dic = {}\n",
        "    dic['text'] = my_dataset[i]\n",
        "    list_dict.append(dic)\n",
        "\n",
        "# make list into jsonl format\n",
        "import json\n",
        "with open('my_dataset.jsonl', 'w', encoding='utf-8') as outfile:\n",
        "    for entry in list_dict:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')"
      ],
      "metadata": {
        "id": "hgYBSN2u1Wlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer  \n",
        "There are three options for tokenizer.\n",
        "1. You can download English tokenizers from the link below.\n",
        "2. You can use any tokenizer you want, as long as they are HFGPT2Tokenizer, HFTokenizer, GPT2BPETokenizer, or CharLevelTokenizer.\n",
        "3. You can make custom tokenizer with the code below.   \n",
        "\n",
        "GPT2 Tokenizer\n",
        "- Vocab: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
        "- Merge: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
        "\n",
        "GPT-NeoX 20B Tokenizer  \n",
        "https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/20B_tokenizer.json  \n"
      ],
      "metadata": {
        "id": "aLOCyw-N1rFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \\\n",
        "\t\t\t\t\t\t\t\t\t\t\tvocab_size = 60000, min_frequency = 5)\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "files = [\"./data/tokenizer_data.txt\"]  # has to be txt file\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "awesome_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "awesome_tokenizer.save_pretrained(\"./data\")"
      ],
      "metadata": {
        "id": "5RmO_R94oNBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed0afca-f95d-4dbb-c252-104f1c4aa00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./data/tokenizer_config.json',\n",
              " './data/special_tokens_map.json',\n",
              " './data/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using custom tokenizer, fix gpt-neox/tools/datasets/preprocess_data.py file as below.  \n",
        "\n",
        "```\n",
        "for key, sentences in doc.items():\n",
        "    sentence = list(filter(None.__ne__, sentence))  # add this on 223rd line\n",
        "        for sentence in sentences:\n",
        "            builders[key].add_item(np.array(sentence, dtype=builders[key].dtype))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cCc4XPc_26LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "Tokenize jsonl file and make into two files, bin and idx."
      ],
      "metadata": {
        "id": "BocmsMJ23xuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/datasets/preprocess_data.py \\\n",
        "            --input /content/gpt-neox/data/my_dataset.jsonl \\\n",
        "            --output-prefix ./data/mydataset \\\n",
        "            --vocab /content/gpt-neox/data/tokenizer.json \\\n",
        "            --dataset-impl mmap \\\n",
        "            --tokenizer-type HFTokenizer \\\n",
        "            --append-eod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvCXGv2VX4NV",
        "outputId": "6f4b3ac2-ce64-4768-f092-78d33ec02a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "> building HFTokenizer tokenizer ...\n",
            " > padded vocab (size: 60000) with 32 dummy tokens (new size: 60032)\n",
            "Vocab size: 60000\n",
            "Output prefix: ./data/mydataset\n",
            "> building HFTokenizer tokenizer ...\n",
            " > padded vocab (size: 60000) with 32 dummy tokens (new size: 60032)\n",
            "Processed 6900 documents (2026.03 docs/s, 0.80 MB/s).: : 6900it [00:03, 2010.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "hy13mDtM5h1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent ninja build error"
      ],
      "metadata": {
        "id": "j1PSbdLj4k3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqYExi_JZVMs",
        "outputId": "270fd5fc-4c38-43d7-9919-48c969c96cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 05:21:55--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231123T052155Z&X-Amz-Expires=300&X-Amz-Signature=9f32555075b13273ded23ed1ff4baef683543c10ba01ad44a98f6e596e8ba8fd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-23 05:21:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231123T052155Z&X-Amz-Expires=300&X-Amz-Signature=9f32555075b13273ded23ed1ff4baef683543c10ba01ad44a98f6e596e8ba8fd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-23 05:21:55 (5.12 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix Configuration Files"
      ],
      "metadata": {
        "id": "zZl8qZyg4xOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix two configuration files, local_setup.yml and your_chosen_parameter_size.yml.  \n",
        "**local_setup.yml**\n",
        "\n",
        "```\n",
        "{\n",
        "  \"data_path\": \"data/mydataset_text_document\",\n",
        "\n",
        "  \"save\": \"checkpoints\",\n",
        "  \"load\": \"checkpoints\",\n",
        "  \"checkpoint_validation_with_forward_pass\": False,\n",
        "\n",
        "  \"tensorboard_dir\": \"tensorboard\",\n",
        "  \"log_dir\": \"logs\",\n",
        "  \"use_wandb\": True,\n",
        "  \"wandb_host\": \"https://api.wandb.ai\",\n",
        "  \"wandb_project\": \"neox\"\n",
        "}\n",
        "```\n",
        "**your_chosen_parameter_size.yml**\n",
        "```\n",
        "// highlighted hyperparameters are a must\n",
        "\n",
        "// parallelism settings\n",
        "// multiplication of the numbers should be the number of GPUs\n",
        "\t\"pipe_parallel_size\": 1,\n",
        "\t\"model_parallel_size\": 1,\n",
        "\n",
        "// add tokenizer info\n",
        "\n",
        "\"tokenizer_type\": \"HFTokenizer\",\n",
        "\"vocab_file\": \"data/tokenizer.json\",\n",
        "\"merge_file\": \"data/merges.txt\",\n",
        "\n",
        "\n",
        "// choose iteration size you want\n",
        "// train_iters and lr_decay_iters should be the same\n",
        "// checkpoint factor size you want\n",
        "\n",
        "\"train_iters\": 1000,\n",
        "\"lr_decay_iters\": 1000,\n",
        "\"distributed_backend\": \"nccl\",\n",
        "\"lr_decay_style\": \"cosine\",\n",
        "\"warmup\": 0.01,\n",
        "\"checkpoint_factor\": 500,\n",
        "\"eval_interval\": 100000,\n",
        "\"eval_iters\": 10,\n",
        "```\n"
      ],
      "metadata": {
        "id": "bH37njGP5pB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter explaination\n",
        "- **pipe_parallel_size**: number of pipeline parallel stages, has to do with deepspeed\n",
        "pipeline parallel is partitioning the layers of a model into stages that can be processed in parallel, improves memory and compute efficiency\n",
        "- **model_parallel_size**: size of the model parallelism, has to do with Megatron-LM\n",
        "model parallism is splitting model across multiple devices\n",
        "- **tokenizer_type**: choose between ['GPT2BPETokenizer', 'HFTokenizer', 'HFGPT2Tokenizer', 'SPMTokenizer', 'CharLevelTokenizer', 'TiktokenTokenizer']\n",
        "- **merge_file**: only if you have merge.txt file\n",
        "- **train_iters**: number of iterations to run for training\n",
        "- **lr_decay_iters**: number of iterations to decay learning rate over\n",
        "after every n training iterations, learning rate is adjusted\n",
        "default is train_iters\n",
        "- **lr_decay_style**: learning rate decay function, choose between 'constant', 'linear', 'cosine', 'exponential'\n",
        "- **warmup**: percentage of total iterations to warmup on\n",
        "starting with a small learning rate and gradually increasing it during the training\n",
        "- **checkpoint_factor**: choose between ‘log’ or ‘linear\n",
        "    - log: checkpoint will be saved square of the number\n",
        "    - linear: checkpoint will be saved multiplication of the number\n",
        "- **eval_interval**: interval between running evaluation on validation set\n",
        "- **eval_iters**: number of iterations to run for evaluation validation/test for"
      ],
      "metadata": {
        "id": "f-sAU78v5Sk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solve fused_adam Error\n",
        "\n",
        "change c++14 to c++17 from these three files\n",
        "\n",
        "- /usr/local/lib/python3.10/dist-packages/deepspeed/ops/op_builder/async_io.py\n",
        "- /usr/local/lib/python3.10/dist-packages/deepspeed/ops/op_builder/builder.py\n",
        "- /usr/local/lib/python3.10/dist-packages/deepspeed/ops/op_builder/cpu/builder.py"
      ],
      "metadata": {
        "id": "X8i4AF4y59Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "aHfD7SnD5sGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./deepy.py train.py -d configs your_chosen_parameter_size.yml local_setup.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iO1MXPrZfzf",
        "outputId": "05e35537-0b2f-4848-c483-2014574b73fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "NeoXArgs.from_ymls() ['configs/19M.yml', 'configs/local_setup.yml']\n",
            "INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 1\n",
            "-------------------- arguments --------------------\n",
            "  attention_config ................ ['global', 'global', 'global', 'global', 'global', 'global']updated\n",
            "  attention_dropout ............... 0...........................updated\n",
            "  batch_size ...................... 4...........................updated\n",
            "  checkpoint_activations .......... True........................updated\n",
            "  checkpoint_factor ............... 5...........................updated\n",
            "  clip_grad ....................... 1.0.........................updated\n",
            "  config_files .................... {'19M.yml': '{\\n  \"pipe_parallel_size\": 1,\\n  \"model_parallel_size\": 1,\\n\\n  # model settings\\n  \"num_layers\": 6,\\n  \"hidden_size\": 512,\\n  \"num_attention_heads\": 8,\\n  \"seq_length\": 2048,\\n  \"max_position_embeddings\": 2048,\\n  \"pos_emb\": \"rotary\",\\n  \"no_weight_tying\": true,\\n  \"gpt_j_residual\": false,\\n  \"output_layer_parallelism\": \"column\",\\n\\n  \"tokenizer_type\": \"HFTokenizer\",\\n\"vocab_file\": \"data/tokenizer.json\",\\n\"merge_file\": \"data/merges.txt\",\\n\\n  \"scaled_upper_triang_masked_softmax_fusion\": false,\\n  \"bias_gelu_fusion\": false,\\n\\n  # init methods\\n  \"init_method\": \"small_init\",\\n  \"output_layer_init_method\": \"wang_init\",\\n\\n  \"optimizer\": {\\n    \"type\": \"Adam\",\\n    \"params\": {\\n      \"lr\": 0.001,\\n      \"betas\": [0.9, 0.95],\\n      \"eps\": 1.0e-8,\\n    }\\n  },\\n  \"min_lr\": 0.0001,\\n\\n  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\\n  \"zero_optimization\": {\\n    \"stage\": 1,\\n    \"allgather_partitions\": True,\\n    \"allgather_bucket_size\": 500000000,\\n    \"overlap_comm\": True,\\n    \"reduce_scatter\": True,\\n    \"reduce_bucket_size\": 500000000,\\n    \"contiguous_gradients\": True,\\n  },\\n\\n  \"train_micro_batch_size_per_gpu\": 4, #32,\\n  \"gas\": 1,\\n  \"data_impl\": \"mmap\",\\n  \"num_workers\": 1,\\n\\n  # activation checkpointing\\n  \"checkpoint_activations\": true,\\n  \"checkpoint_num_layers\": 1,\\n  \"partition_activations\": true,\\n  \"synchronize_each_layer\": true,\\n\\n  # regularization\\n  \"gradient_clipping\": 1.0,\\n  \"weight_decay\": 0.1,\\n  \"hidden_dropout\": 0,\\n  \"attention_dropout\": 0,\\n\\n  # precision settings\\n  \"fp16\": {\\n    \"fp16\": true,\\n    \"enabled\": true,\\n    \"loss_scale\": 0,\\n    \"loss_scale_window\": 1000,\\n    \"initial_scale_power\": 12,\\n    \"hysteresis\": 2,\\n    \"min_loss_scale\": 1,\\n  },\\n\\n  \"train_iters\": 10,\\n  \"lr_decay_iters\": 10,\\n  \"distributed_backend\": \"nccl\",\\n  \"lr_decay_style\": \"cosine\",\\n  \"warmup\": 0.01,\\n  \"checkpoint_factor\": 5,\\n  \"eval_interval\": 100000,\\n  \"eval_iters\": 10,\\n\\n  \"log_interval\": 10,\\n  \"steps_per_print\": 10,\\n  \"wall_clock_breakdown\": true,\\n\\n  # additional deepspeed args not specified above\\n  \"deepspeed_extra_args\": {\\n    \"comms_logger\": {\\n        \"enabled\": true,\\n        \"verbose\": true,\\n        \"prof_all\": true,\\n        \"debug\": false\\n    },\\n  }\\n\\n}\\n', 'local_setup.yml': '{\\n  \"data_path\": \"data/mydataset_text_document\",\\n\\n  \"save\": \"checkpoints\",\\n  \"load\": \"checkpoints\",\\n  \"checkpoint_validation_with_forward_pass\": False,\\n\\n  \"tensorboard_dir\": \"tensorboard\",\\n  \"log_dir\": \"logs\",\\n  \"use_wandb\": True,\\n  \"wandb_host\": \"https://api.wandb.ai\",\\n  \"wandb_project\": \"neox\"\\n}'}updated\n",
            "  data_impl ....................... mmap........................updated\n",
            "  data_path ....................... data/mydataset_text_documentupdated\n",
            "  deepspeed_extra_args ............ {'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}updated\n",
            "  dynamic_loss_scale .............. True........................updated\n",
            "  eval_interval ................... 100000......................updated\n",
            "  eval_iters ...................... 10..........................updated\n",
            "  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated\n",
            "  gas ............................. 1...........................updated\n",
            "  global_num_gpus ................. 1...........................updated\n",
            "  hidden_dropout .................. 0...........................updated\n",
            "  hidden_size ..................... 512.........................updated\n",
            "  init_method ..................... small_init..................updated\n",
            "  is_pipe_parallel ................ True........................updated\n",
            "  load ............................ checkpoints.................updated\n",
            "  log_dir ......................... logs........................updated\n",
            "  log_interval .................... 10..........................updated\n",
            "  lr .............................. 0.001.......................updated\n",
            "  lr_decay_iters .................. 10..........................updated\n",
            "  lr_decay_style .................. cosine......................updated\n",
            "  max_position_embeddings ......... 2048........................updated\n",
            "  merge_file ...................... data/merges.txt.............updated\n",
            "  min_lr .......................... 0.0001......................updated\n",
            "  no_weight_tying ................. True........................updated\n",
            "  num_attention_heads ............. 8...........................updated\n",
            "  num_layers ...................... 6...........................updated\n",
            "  num_workers ..................... 1...........................updated\n",
            "  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated\n",
            "  optimizer_type .................. Adam........................updated\n",
            "  output_layer_init_method ........ wang_init...................updated\n",
            "  partition_activations ........... True........................updated\n",
            "  pipe_parallel_size .............. 1...........................updated\n",
            "  pos_emb ......................... rotary......................updated\n",
            "  precision ....................... fp16........................updated\n",
            "  save ............................ checkpoints.................updated\n",
            "  save_iters ...................... [5].........................updated\n",
            "  seq_length ...................... 2048........................updated\n",
            "  sparsity_config ................. {}..........................updated\n",
            "  synchronize_each_layer .......... True........................updated\n",
            "  tensorboard_dir ................. tensorboard.................updated\n",
            "  text_gen_type ................... unconditional...............updated\n",
            "  tokenizer_type .................. HFTokenizer.................updated\n",
            "  train_batch_size ................ 4...........................updated\n",
            "  train_iters ..................... 10..........................updated\n",
            "  train_micro_batch_size_per_gpu .. 4...........................updated\n",
            "  use_wandb ....................... True........................updated\n",
            "  user_script ..................... train.py....................updated\n",
            "  vocab_file ...................... data/tokenizer.json.........updated\n",
            "  wall_clock_breakdown ............ True........................updated\n",
            "  wandb_group ..................... n1pg6tzv_svsin5w4...........updated\n",
            "  weight_decay .................... 0.1.........................updated\n",
            "  zero_allgather_bucket_size ...... 500000000...................updated\n",
            "  zero_contiguous_gradients ....... True........................updated\n",
            "  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}updated\n",
            "  zero_reduce_bucket_size ......... 500000000...................updated\n",
            "  zero_reduce_scatter ............. True........................updated\n",
            "  zero_stage ...................... 1...........................updated\n",
            "  activation ...................... gelu........................default\n",
            "  activation_checkpointing ........ None........................default\n",
            "  adlr_autoresume ................. False.......................default\n",
            "  adlr_autoresume_interval ........ 1000........................default\n",
            "  amp ............................. None........................default\n",
            "  apply_query_key_layer_scaling ... False.......................default\n",
            "  attention_softmax_in_fp32 ....... False.......................default\n",
            "  autotuning ...................... None........................default\n",
            "  autotuning_run .................. None........................default\n",
            "  base_shapes_file ................ None........................default\n",
            "  bf16 ............................ None........................default\n",
            "  bias_dropout_fusion ............. False.......................default\n",
            "  bias_gelu_fusion ................ False.......................default\n",
            "  char_level_ppl .................. False.......................default\n",
            "  checkpoint ...................... None........................default\n",
            "  checkpoint_in_cpu ............... False.......................default\n",
            "  checkpoint_num_layers ........... 1...........................default\n",
            "  checkpoint_scale ................ linear......................default\n",
            "  checkpoint_validation_with_forward_pass  False................default\n",
            "  comment ......................... None........................default\n",
            "  comms_logger .................... None........................default\n",
            "  communication_data_type ......... None........................default\n",
            "  compression_training ............ None........................default\n",
            "  contiguous_checkpointing ........ False.......................default\n",
            "  coord_check ..................... False.......................default\n",
            "  csv_monitor ..................... None........................default\n",
            "  curriculum_learning ............. None........................default\n",
            "  curriculum_seqlen ............... 0...........................default\n",
            "  data_efficiency ................. None........................default\n",
            "  data_types ...................... None........................default\n",
            "  deepscale ....................... False.......................default\n",
            "  deepscale_config ................ None........................default\n",
            "  deepspeed ....................... True........................default\n",
            "  deepspeed_activation_checkpointing  True......................default\n",
            "  deepspeed_mpi ................... False.......................default\n",
            "  deepspeed_slurm ................. False.......................default\n",
            "  detect_nvlink_pairs ............. False.......................default\n",
            "  distributed_backend ............. nccl........................default\n",
            "  do_test ......................... None........................default\n",
            "  do_train ........................ None........................default\n",
            "  do_valid ........................ None........................default\n",
            "  dump_state ...................... False.......................default\n",
            "  elasticity ...................... None........................default\n",
            "  eod_mask_loss ................... False.......................default\n",
            "  eval_results_prefix ............. ............................default\n",
            "  eval_tasks ...................... None........................default\n",
            "  exclude ......................... None........................default\n",
            "  exit_interval ................... None........................default\n",
            "  extra_save_iters ................ None........................default\n",
            "  finetune ........................ False.......................default\n",
            "  flops_profiler .................. None........................default\n",
            "  force_multi ..................... False.......................default\n",
            "  fp16_lm_cross_entropy ........... False.......................default\n",
            "  fp32_allreduce .................. False.......................default\n",
            "  git_hash ........................ f48d3a6.....................default\n",
            "  gmlp_attn_dim ................... 64..........................default\n",
            "  gpt_j_residual .................. False.......................default\n",
            "  gpt_j_tied ...................... False.......................default\n",
            "  gradient_accumulation_steps ..... 1...........................default\n",
            "  gradient_clipping ............... 1.0.........................default\n",
            "  gradient_noise_scale_cpu_offload  False.......................default\n",
            "  gradient_noise_scale_n_batches .. 5...........................default\n",
            "  gradient_predivide_factor ....... 1.0.........................default\n",
            "  hostfile ........................ None........................default\n",
            "  hysteresis ...................... 2...........................default\n",
            "  include ......................... None........................default\n",
            "  init_method_std ................. 0.02........................default\n",
            "  iteration ....................... None........................default\n",
            "  keep_last_n_checkpoints ......... None........................default\n",
            "  label_data_paths ................ None........................default\n",
            "  launcher ........................ pdsh........................default\n",
            "  layernorm_epsilon ............... 1e-05.......................default\n",
            "  lazy_mpu_init ................... False.......................default\n",
            "  local_rank ...................... None........................default\n",
            "  log_grad_norm ................... False.......................default\n",
            "  log_grad_pct_zeros .............. False.......................default\n",
            "  log_gradient_noise_scale ........ False.......................default\n",
            "  log_optimizer_states ............ False.......................default\n",
            "  log_param_norm .................. False.......................default\n",
            "  loss_scale ...................... None........................default\n",
            "  loss_scale_window ............... 1000.0......................default\n",
            "  make_vocab_size_divisible_by .... 128.........................default\n",
            "  master_addr ..................... None........................default\n",
            "  master_port ..................... 29500.......................default\n",
            "  maximum_tokens .................. 64..........................default\n",
            "  min_scale ....................... 1.0.........................default\n",
            "  mlp_type ........................ regular.....................default\n",
            "  mmap_warmup ..................... False.......................default\n",
            "  model_parallel_size ............. 1...........................default\n",
            "  mup_attn_temp ................... 1.0.........................default\n",
            "  mup_embedding_mult .............. 1.0.........................default\n",
            "  mup_init_scale .................. 1.0.........................default\n",
            "  mup_output_temp ................. 1.0.........................default\n",
            "  mup_rp_embedding_mult ........... 1.0.........................default\n",
            "  mup_width_scale ................. 2...........................default\n",
            "  no_load_optim ................... False.......................default\n",
            "  no_load_rng ..................... False.......................default\n",
            "  no_save_optim ................... False.......................default\n",
            "  no_save_rng ..................... False.......................default\n",
            "  no_ssh_check .................... False.......................default\n",
            "  norm ............................ layernorm...................default\n",
            "  num_gpus ........................ None........................default\n",
            "  num_nodes ....................... -1..........................default\n",
            "  num_samples ..................... 1...........................default\n",
            "  num_unique_layers ............... None........................default\n",
            "  onnx_safe ....................... False.......................default\n",
            "  opt_pos_emb_offset .............. 0...........................default\n",
            "  output_layer_parallelism ........ column......................default\n",
            "  override_lr_scheduler ........... False.......................default\n",
            "  padded_vocab_size ............... None........................default\n",
            "  param_sharing_style ............. grouped.....................default\n",
            "  pipe_partition_method ........... type:transformer|mlp........default\n",
            "  prescale_gradients .............. False.......................default\n",
            "  profile_backward ................ False.......................default\n",
            "  prompt_end ...................... \n",
            "...........................default\n",
            "  rank ............................ None........................default\n",
            "  recompute ....................... False.......................default\n",
            "  return_logits ................... False.......................default\n",
            "  rms_norm_epsilon ................ 1e-08.......................default\n",
            "  rotary_emb_base ................. 10000.......................default\n",
            "  rotary_pct ...................... 1.0.........................default\n",
            "  rpe_max_distance ................ 128.........................default\n",
            "  rpe_num_buckets ................. 32..........................default\n",
            "  s3_chunk_size ................... 104857600...................default\n",
            "  s3_path ......................... None........................default\n",
            "  sample_input_file ............... None........................default\n",
            "  sample_output_file .............. samples.txt.................default\n",
            "  save_base_shapes ................ False.......................default\n",
            "  scaled_masked_softmax_fusion .... False.......................default\n",
            "  scaled_upper_triang_masked_softmax_fusion  False..............default\n",
            "  scalenorm_epsilon ............... 1e-08.......................default\n",
            "  scheduler ....................... None........................default\n",
            "  seed ............................ 1234........................default\n",
            "  short_seq_prob .................. 0.1.........................default\n",
            "  soft_prompt_tuning .............. None........................default\n",
            "  sparse_attention ................ None........................default\n",
            "  sparse_gradients ................ False.......................default\n",
            "  split ........................... 969, 30, 1..................default\n",
            "  steps_per_print ................. 10..........................default\n",
            "  temperature ..................... 0.0.........................default\n",
            "  tensorboard ..................... None........................default\n",
            "  test_data_paths ................. None........................default\n",
            "  test_data_weights ............... None........................default\n",
            "  top_k ........................... 0...........................default\n",
            "  top_p ........................... 0.0.........................default\n",
            "  train_data_paths ................ None........................default\n",
            "  train_data_weights .............. None........................default\n",
            "  use_bias_in_attn_linear ......... True........................default\n",
            "  use_bias_in_norms ............... True........................default\n",
            "  use_bnb_optimizer ............... False.......................default\n",
            "  use_checkpoint_lr_scheduler ..... False.......................default\n",
            "  use_cpu_initialization .......... False.......................default\n",
            "  use_mup ......................... False.......................default\n",
            "  use_shared_fs ................... True........................default\n",
            "  valid_data_paths ................ None........................default\n",
            "  valid_data_weights .............. None........................default\n",
            "  wandb ........................... None........................default\n",
            "  wandb_host ...................... https://api.wandb.ai........default\n",
            "  wandb_init_all_ranks ............ False.......................default\n",
            "  wandb_project ................... neox........................default\n",
            "  wandb_team ...................... None........................default\n",
            "  warmup .......................... 0.01........................default\n",
            "  weight_by_num_documents ......... False.......................default\n",
            "  weighted_sampler_alpha .......... 0.3.........................default\n",
            "  world_size ...................... None........................default\n",
            "---------------- end of arguments ----------------\n",
            "NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 \n",
            "[2023-11-23 06:04:40,457] [WARNING] [runner.py:199:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2023-11-23 06:04:40,458] [INFO] [runner.py:553:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDEsICJiZXRhcyI6IFswLjksIDAuOTVdLCAiZXBzIjogMWUtMDh9fSwgImZwMTYiOiB7ImZwMTYiOiB0cnVlLCAiZW5hYmxlZCI6IHRydWUsICJsb3NzX3NjYWxlIjogMCwgImxvc3Nfc2NhbGVfd2luZG93IjogMTAwMCwgImluaXRpYWxfc2NhbGVfcG93ZXIiOiAxMiwgImh5c3RlcmVzaXMiOiAyLCAibWluX2xvc3Nfc2NhbGUiOiAxfSwgInplcm9fb3B0aW1pemF0aW9uIjogeyJzdGFnZSI6IDEsICJhbGxnYXRoZXJfcGFydGl0aW9ucyI6IHRydWUsICJhbGxnYXRoZXJfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJvdmVybGFwX2NvbW0iOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAicmVkdWNlX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAiY29udGlndW91c19ncmFkaWVudHMiOiB0cnVlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ== --megatron_config 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\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-11-23 06:04:43,630] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3\n",
            "For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer\n",
            "2023-11-23 06:04:49.885902: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-23 06:04:49.885966: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-23 06:04:49.886011: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-23 06:04:51.662405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 \n",
            "> building HFTokenizer tokenizer ...\n",
            " > padded vocab (size: 60000) with 32 dummy tokens (new size: 60032)\n",
            "> setting tensorboard ...\n",
            "> initializing torch distributed ...\n",
            "[2023-11-23 06:04:52,922] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-11-23 06:04:52,923] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-11-23 06:04:52,923] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "> initializing model parallel with size 1\n",
            "MPU DP: [0]\n",
            "MPU PP: [0]\n",
            "MPU MP: [0]\n",
            "> setting random seeds to 1234 ...\n",
            "[2023-11-23 06:04:52,925] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
            "make: Entering directory '/content/gpt-neox/megatron/data'\n",
            "make: Nothing to be done for 'default'.\n",
            "make: Leaving directory '/content/gpt-neox/megatron/data'\n",
            "building GPT2 model ...\n",
            "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
            "Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0}\n",
            "[2023-11-23 06:04:53,082] [INFO] [module.py:358:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp\n",
            "stage=0 layers=11\n",
            "     0: EmbeddingPipe\n",
            "     1: _pre_transformer_block\n",
            "     2: ParallelTransformerLayerPipe\n",
            "     3: ParallelTransformerLayerPipe\n",
            "     4: ParallelTransformerLayerPipe\n",
            "     5: ParallelTransformerLayerPipe\n",
            "     6: ParallelTransformerLayerPipe\n",
            "     7: ParallelTransformerLayerPipe\n",
            "     8: _post_transformer_block\n",
            "     9: NormPipe\n",
            "    10: ParallelLinearPipe\n",
            "  loss: partial\n",
            "Configuring Optimizer type: Adam with params: {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
            "WARNING: APEX not installed - defaulting to deepspeed's fused adam\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.008424758911132812 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
            "> learning rate decay style: cosine\n",
            "DeepSpeed is enabled.\n",
            "[2023-11-23 06:04:56,368] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.3+a48c649, git-hash=a48c649, git-branch=main\n",
            "[2023-11-23 06:04:56,412] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 39.21 | msg size: 58.62 MB | algbw (Gbps): 12.54  | busbw (Gbps): 12.54 \n",
            "[2023-11-23 06:04:56,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.20 | msg size: 1.0 KB | algbw (Gbps): 0.04  | busbw (Gbps): 0.04 \n",
            "[2023-11-23 06:04:56,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,414] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.5 MB | algbw (Gbps): 97.02  | busbw (Gbps): 97.02 \n",
            "[2023-11-23 06:04:56,415] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 3.0 KB | algbw (Gbps): 0.18  | busbw (Gbps): 0.18 \n",
            "[2023-11-23 06:04:56,415] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 512.0 KB | algbw (Gbps): 34.13  | busbw (Gbps): 34.13 \n",
            "[2023-11-23 06:04:56,416] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,416] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,417] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 1.0 KB | algbw (Gbps): 0.05  | busbw (Gbps): 0.05 \n",
            "[2023-11-23 06:04:56,417] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 128.38  | busbw (Gbps): 128.38 \n",
            "[2023-11-23 06:04:56,418] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 KB | algbw (Gbps): 0.27  | busbw (Gbps): 0.27 \n",
            "[2023-11-23 06:04:56,418] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 2.0 MB | algbw (Gbps): 138.85  | busbw (Gbps): 138.85 \n",
            "[2023-11-23 06:04:56,418] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,419] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,419] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.20 | msg size: 1.5 MB | algbw (Gbps): 62.73  | busbw (Gbps): 62.73 \n",
            "[2023-11-23 06:04:56,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.18 | msg size: 3.0 KB | algbw (Gbps): 0.14  | busbw (Gbps): 0.14 \n",
            "[2023-11-23 06:04:56,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 512.0 KB | algbw (Gbps): 32.46  | busbw (Gbps): 32.46 \n",
            "[2023-11-23 06:04:56,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,422] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 1.0 KB | algbw (Gbps): 0.05  | busbw (Gbps): 0.05 \n",
            "[2023-11-23 06:04:56,422] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,423] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 124.62  | busbw (Gbps): 124.62 \n",
            "[2023-11-23 06:04:56,423] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 KB | algbw (Gbps): 0.28  | busbw (Gbps): 0.28 \n",
            "[2023-11-23 06:04:56,424] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 2.0 MB | algbw (Gbps): 134.30  | busbw (Gbps): 134.30 \n",
            "[2023-11-23 06:04:56,424] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,424] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,425] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,425] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.5 MB | algbw (Gbps): 107.76  | busbw (Gbps): 107.76 \n",
            "[2023-11-23 06:04:56,426] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 3.0 KB | algbw (Gbps): 0.19  | busbw (Gbps): 0.19 \n",
            "[2023-11-23 06:04:56,426] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 512.0 KB | algbw (Gbps): 33.57  | busbw (Gbps): 33.57 \n",
            "[2023-11-23 06:04:56,426] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,427] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,427] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,428] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 130.03  | busbw (Gbps): 130.03 \n",
            "[2023-11-23 06:04:56,428] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 KB | algbw (Gbps): 0.28  | busbw (Gbps): 0.28 \n",
            "[2023-11-23 06:04:56,429] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 126.88  | busbw (Gbps): 126.88 \n",
            "[2023-11-23 06:04:56,429] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,429] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,430] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,430] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.5 MB | algbw (Gbps): 107.52  | busbw (Gbps): 107.52 \n",
            "[2023-11-23 06:04:56,431] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 3.0 KB | algbw (Gbps): 0.18  | busbw (Gbps): 0.18 \n",
            "[2023-11-23 06:04:56,431] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 512.0 KB | algbw (Gbps): 35.31  | busbw (Gbps): 35.31 \n",
            "[2023-11-23 06:04:56,432] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,432] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,432] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,433] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 133.44  | busbw (Gbps): 133.44 \n",
            "[2023-11-23 06:04:56,434] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.24 | msg size: 4.0 KB | algbw (Gbps): 0.13  | busbw (Gbps): 0.13 \n",
            "[2023-11-23 06:04:56,434] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 2.0 MB | algbw (Gbps): 133.99  | busbw (Gbps): 133.99 \n",
            "[2023-11-23 06:04:56,435] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.22 | msg size: 1.0 KB | algbw (Gbps): 0.04  | busbw (Gbps): 0.04 \n",
            "[2023-11-23 06:04:56,435] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.20 | msg size: 1.0 KB | algbw (Gbps): 0.04  | busbw (Gbps): 0.04 \n",
            "[2023-11-23 06:04:56,436] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 1.0 KB | algbw (Gbps): 0.05  | busbw (Gbps): 0.05 \n",
            "[2023-11-23 06:04:56,436] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.5 MB | algbw (Gbps): 105.34  | busbw (Gbps): 105.34 \n",
            "[2023-11-23 06:04:56,437] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 3.0 KB | algbw (Gbps): 0.20  | busbw (Gbps): 0.20 \n",
            "[2023-11-23 06:04:56,437] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 512.0 KB | algbw (Gbps): 33.59  | busbw (Gbps): 33.59 \n",
            "[2023-11-23 06:04:56,437] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,438] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,438] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,439] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 2.0 MB | algbw (Gbps): 116.56  | busbw (Gbps): 116.56 \n",
            "[2023-11-23 06:04:56,439] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 KB | algbw (Gbps): 0.28  | busbw (Gbps): 0.28 \n",
            "[2023-11-23 06:04:56,439] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 2.0 MB | algbw (Gbps): 142.08  | busbw (Gbps): 142.08 \n",
            "[2023-11-23 06:04:56,440] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,440] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.5 MB | algbw (Gbps): 107.58  | busbw (Gbps): 107.58 \n",
            "[2023-11-23 06:04:56,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 3.0 KB | algbw (Gbps): 0.21  | busbw (Gbps): 0.21 \n",
            "[2023-11-23 06:04:56,442] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 512.0 KB | algbw (Gbps): 34.61  | busbw (Gbps): 34.61 \n",
            "[2023-11-23 06:04:56,442] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,443] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,444] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 2.0 MB | algbw (Gbps): 113.36  | busbw (Gbps): 113.36 \n",
            "[2023-11-23 06:04:56,444] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 KB | algbw (Gbps): 0.28  | busbw (Gbps): 0.28 \n",
            "[2023-11-23 06:04:56,444] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 2.0 MB | algbw (Gbps): 144.79  | busbw (Gbps): 144.79 \n",
            "[2023-11-23 06:04:56,445] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,445] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 1.0 KB | algbw (Gbps): 0.07  | busbw (Gbps): 0.07 \n",
            "[2023-11-23 06:04:56,446] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 1.0 KB | algbw (Gbps): 0.06  | busbw (Gbps): 0.06 \n",
            "[2023-11-23 06:04:56,446] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 58.62 MB | algbw (Gbps): 4195.52  | busbw (Gbps): 4195.52 \n",
            "[2023-11-23 06:04:56,446] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2023-11-23 06:04:56,447] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
            "[2023-11-23 06:04:56,447] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2023-11-23 06:04:56,448] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2023-11-23 06:04:56,448] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
            "[2023-11-23 06:04:56,448] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\n",
            "[2023-11-23 06:04:56,448] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000\n",
            "[2023-11-23 06:04:56,448] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000\n",
            "[2023-11-23 06:04:56,448] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False\n",
            "[2023-11-23 06:04:56,448] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.008941888809204102 seconds\n",
            "Rank: 0 partition count [1, 1] and sizes[(80347136, False), (40960, False)] \n",
            "[2023-11-23 06:04:56,760] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 32.39 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:57,017] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
            "[2023-11-23 06:04:57,017] [INFO] [utils.py:786:see_memory_usage] MA 0.45 GB         Max_MA 0.45 GB         CA 0.45 GB         Max_CA 0 GB \n",
            "[2023-11-23 06:04:57,018] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.04 GB, percent = 31.9%\n",
            "[2023-11-23 06:04:57,280] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
            "[2023-11-23 06:04:57,281] [INFO] [utils.py:786:see_memory_usage] MA 1.05 GB         Max_MA 1.35 GB         CA 1.36 GB         Max_CA 1 GB \n",
            "[2023-11-23 06:04:57,281] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.04 GB, percent = 31.9%\n",
            "[2023-11-23 06:04:57,281] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n",
            "[2023-11-23 06:04:57,543] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2023-11-23 06:04:57,544] [INFO] [utils.py:786:see_memory_usage] MA 1.05 GB         Max_MA 1.05 GB         CA 1.36 GB         Max_CA 1 GB \n",
            "[2023-11-23 06:04:57,544] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.05 GB, percent = 31.9%\n",
            "[2023-11-23 06:04:57,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\n",
            "[2023-11-23 06:04:57,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2023-11-23 06:04:57,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7ed055e777c0>\n",
            "[2023-11-23 06:04:57,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]\n",
            "[2023-11-23 06:04:57,547] [INFO] [config.py:958:print] DeepSpeedEngine configuration:\n",
            "[2023-11-23 06:04:57,547] [INFO] [config.py:962:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2023-11-23 06:04:57,547] [INFO] [config.py:962:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2023-11-23 06:04:57,547] [INFO] [config.py:962:print]   amp_enabled .................. False\n",
            "[2023-11-23 06:04:57,547] [INFO] [config.py:962:print]   amp_params ................... False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   bfloat16_enabled ............. False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   checkpoint_tag_validation_enabled  True\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   checkpoint_tag_validation_fail  False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ed055e77970>\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   communication_data_type ...... None\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   curriculum_enabled_legacy .... False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   curriculum_params_legacy ..... False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   data_efficiency_enabled ...... False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   dataloader_drop_last ......... False\n",
            "[2023-11-23 06:04:57,548] [INFO] [config.py:962:print]   disable_allgather ............ False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   dump_state ................... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_enabled ........... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_layer_num ......... 0\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_max_iter .......... 100\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_stability ......... 1e-06\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_tol ............... 0.01\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   eigenvalue_verbose ........... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   elasticity_enabled ........... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   fp16_auto_cast ............... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   fp16_enabled ................. True\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   fp16_master_weights_and_gradients  False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   global_rank .................. 0\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   grad_accum_dtype ............. None\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   gradient_accumulation_steps .. 1\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   gradient_clipping ............ 0.0\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   gradient_predivide_factor .... 1.0\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   initial_dynamic_scale ........ 4096\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   load_universal_checkpoint .... False\n",
            "[2023-11-23 06:04:57,549] [INFO] [config.py:962:print]   loss_scale ................... 0\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   memory_breakdown ............. False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   mics_hierarchial_params_gather  False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   mics_shard_size .............. -1\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   optimizer_legacy_fusion ...... False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   optimizer_name ............... adam\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.95], 'eps': 1e-08}\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   pld_enabled .................. False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   pld_params ................... False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   prescale_gradients ........... False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   scheduler_name ............... None\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   scheduler_params ............. None\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   sparse_attention ............. None\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   sparse_gradients_enabled ..... False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   steps_per_print .............. 10\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   train_batch_size ............. 4\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   train_micro_batch_size_per_gpu  4\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   use_node_local_storage ....... False\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   wall_clock_breakdown ......... True\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   world_size ................... 1\n",
            "[2023-11-23 06:04:57,550] [INFO] [config.py:962:print]   zero_allow_untested_optimizer  False\n",
            "[2023-11-23 06:04:57,551] [INFO] [config.py:962:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
            "[2023-11-23 06:04:57,551] [INFO] [config.py:962:print]   zero_enabled ................. True\n",
            "[2023-11-23 06:04:57,551] [INFO] [config.py:962:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2023-11-23 06:04:57,551] [INFO] [config.py:962:print]   zero_optimization_stage ...... 1\n",
            "[2023-11-23 06:04:57,551] [INFO] [config.py:948:print_user_config]   json = {\n",
            "    \"train_batch_size\": 4, \n",
            "    \"train_micro_batch_size_per_gpu\": 4, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"params\": {\n",
            "            \"lr\": 0.001, \n",
            "            \"betas\": [0.9, 0.95], \n",
            "            \"eps\": 1e-08\n",
            "        }\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"fp16\": true, \n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 12, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 1, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 5.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 5.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"wall_clock_breakdown\": true, \n",
            "    \"comms_logger\": {\n",
            "        \"enabled\": true, \n",
            "        \"verbose\": true, \n",
            "        \"prof_all\": true, \n",
            "        \"debug\": false\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0003609657287597656 seconds\n",
            "[2023-11-23 06:04:57,551] [INFO] [engine.py:83:__init__] CONFIG: micro_batches=1 micro_batch_size=4\n",
            "[2023-11-23 06:04:57,587] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 35.05 | msg size: 16.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:57,588] [INFO] [engine.py:138:__init__] RANK=0 STAGE=0 LAYERS=11 [0, 11) STAGE_PARAMS=80388096 (80.388M) TOTAL_PARAMS=80388096 (80.388M) UNIQUE_PARAMS=80388096 (80.388M)\n",
            " > number of parameters on model parallel rank 0: 80388096\n",
            " > total params: 80,388,096\n",
            "[2023-11-23 06:04:57,589] [WARNING] [engine.py:2589:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.\n",
            "Unable to load checkpoint.\n",
            "Loading checkpoint and starting from iteration 0\n",
            "> building train, validation, and test datasets ...\n",
            "    reading sizes...\n",
            "    reading pointers...\n",
            "    reading document index...\n",
            "    creating numpy buffer of mmap...\n",
            "    creating memory view of numpy buffer...\n",
            " > dataset split:\n",
            "    train:\n",
            "     document indices in [0, 6733) total of 6733 documents\n",
            "    validation:\n",
            "     document indices in [6733, 6941) total of 208 documents\n",
            "    test:\n",
            "     document indices in [6941, 6948) total of 7 documents\n",
            " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
            " > elapsed time to build and save doc-idx mapping (seconds): 0.000597\n",
            "    using:\n",
            "     number of documents:       6733\n",
            "     number of epochs:          1\n",
            "     sequence length:           2048\n",
            "     total number of samples:   348\n",
            " > elapsed time to build and save sample-idx mapping (seconds): 0.000977\n",
            " > elapsed time to build and save shuffle-idx mapping (seconds): 0.000164\n",
            " > loading doc-idx mapping from data/mydataset_text_document_train_indexmap_40ns_2048sl_1234s_doc_idx.npy\n",
            " > loading sample-idx mapping from data/mydataset_text_document_train_indexmap_40ns_2048sl_1234s_sample_idx.npy\n",
            " > loading shuffle-idx mapping from data/mydataset_text_document_train_indexmap_40ns_2048sl_1234s_shuffle_idx.npy\n",
            "    loaded indexed file in 0.001 seconds\n",
            "    total number of samples: 349\n",
            "    total number of epochs: 1\n",
            " > loading doc-idx mapping from data/mydataset_text_document_valid_indexmap_40ns_2048sl_1234s_doc_idx.npy\n",
            " > loading sample-idx mapping from data/mydataset_text_document_valid_indexmap_40ns_2048sl_1234s_sample_idx.npy\n",
            " > loading shuffle-idx mapping from data/mydataset_text_document_valid_indexmap_40ns_2048sl_1234s_shuffle_idx.npy\n",
            "    loaded indexed file in 0.001 seconds\n",
            "    total number of samples: 41\n",
            "    total number of epochs: 6\n",
            " > loading doc-idx mapping from data/mydataset_text_document_test_indexmap_40ns_2048sl_1234s_doc_idx.npy\n",
            " > loading sample-idx mapping from data/mydataset_text_document_test_indexmap_40ns_2048sl_1234s_sample_idx.npy\n",
            " > loading shuffle-idx mapping from data/mydataset_text_document_test_indexmap_40ns_2048sl_1234s_shuffle_idx.npy\n",
            "    loaded indexed file in 0.001 seconds\n",
            "    total number of samples: 41\n",
            "    total number of epochs: 84\n",
            "setting training data start iteration to 0\n",
            "setting validation data start iteration to 0\n",
            "done with setups ...\n",
            "time (ms) | model and optimizer: 4509.75 | train/valid/test data iterators: 186.37\n",
            "training ...\n",
            "[2023-11-23 06:04:57,826] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information\n",
            "[2023-11-23 06:04:57,826] [INFO] [checkpointing.py:530:forward] ----Partition Activations True, CPU CHECKPOINTING False\n",
            "[2023-11-23 06:04:57,826] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with 6 total layers\n",
            "[2023-11-23 06:04:57,826] [INFO] [checkpointing.py:533:forward] ----Synchronization True\n",
            "[2023-11-23 06:04:57,826] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False\n",
            "[2023-11-23 06:04:59,484] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.04 | msg size: 153.33 MB | algbw (Gbps): 59812.57  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,493] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.20 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,501] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,504] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,527] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.69 | msg size: 153.25 MB | algbw (Gbps): 477.06  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,528] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.23 | msg size: 80.0 KB | algbw (Gbps): 2.81  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:04:59,529] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.64 | optimizer_gradients: 6.82 | optimizer_step: 13.66\n",
            "[2023-11-23 06:04:59,566] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 36.22 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,084] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.06 | msg size: 153.33 MB | algbw (Gbps): 43011.29  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,092] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.22 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,100] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,103] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.16 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,127] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.71 | msg size: 153.25 MB | algbw (Gbps): 474.09  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,127] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.31 | msg size: 80.0 KB | algbw (Gbps): 2.13  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,128] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.75 | optimizer_gradients: 6.81 | optimizer_step: 13.66\n",
            "[2023-11-23 06:05:00,129] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,644] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.05 | msg size: 153.33 MB | algbw (Gbps): 57093.82  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,652] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.21 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,661] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,663] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.29 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,687] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 476.48  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,687] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.26 | msg size: 80.0 KB | algbw (Gbps): 2.52  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:00,689] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.56 | optimizer_gradients: 6.83 | optimizer_step: 13.68\n",
            "[2023-11-23 06:05:00,690] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.23 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,210] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.04 | msg size: 153.33 MB | algbw (Gbps): 59812.57  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,218] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.16 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,226] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.16 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,229] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.14 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,253] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.73 | msg size: 153.25 MB | algbw (Gbps): 471.26  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,254] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.26 | msg size: 80.0 KB | algbw (Gbps): 2.50  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,255] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.83 | optimizer_gradients: 6.85 | optimizer_step: 13.68\n",
            "[2023-11-23 06:05:01,256] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,771] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.05 | msg size: 153.33 MB | algbw (Gbps): 48310.15  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,780] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.28 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,788] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,791] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.19 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,814] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 476.44  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,815] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.21 | msg size: 80.0 KB | algbw (Gbps): 3.14  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,815] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.50 | optimizer_gradients: 6.78 | optimizer_step: 13.67\n",
            "[2023-11-23 06:05:01,816] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,818] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.23 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,818] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5 is about to be saved!\n",
            "[2023-11-23 06:05:01,819] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 20.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,819] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.13 | msg size: 20.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:01,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_00-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_00-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_02-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_02-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_03-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_03-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_04-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_04-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_05-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_05-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_06-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_06-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_07-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_07-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_09-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:01,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_09-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:01,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/layer_10-model_00-model_states.pt...\n",
            "[2023-11-23 06:05:02,137] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/layer_10-model_00-model_states.pt.\n",
            "[2023-11-23 06:05:02,139] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: checkpoints/global_step5/mp_rank_00_model_states.pt\n",
            "[2023-11-23 06:05:02,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/mp_rank_00_model_states.pt...\n",
            "[2023-11-23 06:05:02,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/mp_rank_00_model_states.pt.\n",
            "[2023-11-23 06:05:02,142] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.44 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:02,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving checkpoints/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
            "[2023-11-23 06:05:12,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved checkpoints/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
            "[2023-11-23 06:05:12,689] [INFO] [engine.py:3243:_save_zero_checkpoint] zero checkpoint saved checkpoints/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2023-11-23 06:05:12,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5 is ready now!\n",
            "[2023-11-23 06:05:12,690] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.37 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,276] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.05 | msg size: 153.33 MB | algbw (Gbps): 50590.37  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,284] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,292] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.14 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,295] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,318] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 475.82  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.24 | msg size: 80.0 KB | algbw (Gbps): 2.71  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,320] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.59 | optimizer_gradients: 6.81 | optimizer_step: 13.61\n",
            "[2023-11-23 06:05:13,321] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,837] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.06 | msg size: 153.33 MB | algbw (Gbps): 44859.43  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,845] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,853] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.20 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,857] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.16 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,880] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.71 | msg size: 153.25 MB | algbw (Gbps): 474.10  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,881] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.32 | msg size: 80.0 KB | algbw (Gbps): 2.05  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:13,883] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 4.33 | optimizer_gradients: 6.82 | optimizer_step: 13.57\n",
            "[2023-11-23 06:05:13,884] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,403] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.10 | msg size: 153.33 MB | algbw (Gbps): 26159.48  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,413] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.19 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,424] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,448] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 476.21  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,448] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.24 | msg size: 80.0 KB | algbw (Gbps): 2.78  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,449] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.67 | optimizer_gradients: 6.82 | optimizer_step: 13.72\n",
            "[2023-11-23 06:05:14,450] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.14 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,966] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.05 | msg size: 153.33 MB | algbw (Gbps): 49806.75  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,974] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.20 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,982] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:14,985] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,009] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 476.93  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,009] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.23 | msg size: 80.0 KB | algbw (Gbps): 2.88  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,010] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.53 | optimizer_gradients: 6.82 | optimizer_step: 13.64\n",
            "[2023-11-23 06:05:15,011] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,531] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.06 | msg size: 153.33 MB | algbw (Gbps): 39600.05  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,539] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.18 | msg size: 1.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,547] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,550] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_reduce | time (ms): 0.14 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,573] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 2.70 | msg size: 153.25 MB | algbw (Gbps): 476.10  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,574] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: all_gather | time (ms): 0.22 | msg size: 80.0 KB | algbw (Gbps): 2.94  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,575] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 3.55 | optimizer_gradients: 6.81 | optimizer_step: 13.66\n",
            "[2023-11-23 06:05:15,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001, 0.0001], mom=[[0.9, 0.95], [0.9, 0.95]]\n",
            "[2023-11-23 06:05:15,576] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | batch_input: 68.97 | forward_microstep: 2034.32 | backward_microstep: 3484.57 | backward_inner_microstep: 3483.32 | backward_allreduce_microstep: 0.28 | step_microstep: 431.70\n",
            "[2023-11-23 06:05:15,577] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 2034.11 | backward: 3484.47 | backward_inner: 3483.25 | backward_allreduce: 0.33 | step: 431.86\n",
            "[2023-11-23 06:05:15,578] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 8.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "steps: 10 loss: 8.4402 iter time (s): 0.691 samples/sec: 5.787\n",
            "[2023-11-23 06:05:15,578] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms)\n",
            " samples/sec: 2.247 | iteration       10/      10 | elapsed time per iteration (ms): 1780.1 | learning rate: 1.000E-04 | approx flops per GPU: 1.2TFLOPS | lm_loss: 9.622538E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
            "after 10 iterations memory (MB) | allocated: 2032.52197265625 | max allocated: 4914.6484375 | reserved: 5656.0 | max reserved: 5656.0\n",
            "time (ms)\n",
            "[2023-11-23 06:05:15,581] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.18 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,753] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.19 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,754] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.31 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,929] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.23 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:15,930] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.29 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,104] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.12 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,105] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.25 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,278] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.16 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,279] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.55 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,454] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.13 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,455] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.33 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,629] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.15 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,630] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.33 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,803] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.17 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,804] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.34 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,977] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.18 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:16,978] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.31 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:17,151] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: broadcast | time (ms): 0.21 | msg size: 4.0 B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "[2023-11-23 06:05:17,152] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 | comm op: barrier | time (ms): 0.35 | msg size: 0B | algbw (Gbps): 0.00  | busbw (Gbps): 0.00 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gpt-neox/train.py\", line 27, in <module>\n",
            "    pretrain(neox_args=neox_args)\n",
            "  File \"/content/gpt-neox/megatron/training.py\", line 238, in pretrain\n",
            "    evaluate_and_print_results(\n",
            "  File \"/content/gpt-neox/megatron/training.py\", line 992, in evaluate_and_print_results\n",
            "    total_loss_dict = evaluate(\n",
            "  File \"/content/gpt-neox/megatron/training.py\", line 938, in evaluate\n",
            "    loss = forward_step_fn(\n",
            "  File \"/content/gpt-neox/megatron/training.py\", line 369, in forward_step\n",
            "    return model.eval_batch(data_iterator, return_logits=return_logits)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/pipe/engine.py\", line 421, in eval_batch\n",
            "    self._exec_schedule(sched)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/pipe/engine.py\", line 1307, in _exec_schedule\n",
            "    self._exec_instr(**cmd.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/pipe/engine.py\", line 758, in _exec_load_micro_batch\n",
            "    batch = self._next_batch()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/pipe/engine.py\", line 592, in _next_batch\n",
            "    batch = next(self.data_iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1318, in _next_data\n",
            "    raise StopIteration\n",
            "StopIteration\n",
            "[2023-11-23 06:05:21,669] [INFO] [launch.py:314:sigkill_handler] Killing subprocess 15993\n",
            "[2023-11-23 06:05:21,669] [ERROR] [launch.py:320:sigkill_handler] ['/usr/bin/python3', '-u', 'train.py', '--local_rank=0', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDEsICJiZXRhcyI6IFswLjksIDAuOTVdLCAiZXBzIjogMWUtMDh9fSwgImZwMTYiOiB7ImZwMTYiOiB0cnVlLCAiZW5hYmxlZCI6IHRydWUsICJsb3NzX3NjYWxlIjogMCwgImxvc3Nfc2NhbGVfd2luZG93IjogMTAwMCwgImluaXRpYWxfc2NhbGVfcG93ZXIiOiAxMiwgImh5c3RlcmVzaXMiOiAyLCAibWluX2xvc3Nfc2NhbGUiOiAxfSwgInplcm9fb3B0aW1pemF0aW9uIjogeyJzdGFnZSI6IDEsICJhbGxnYXRoZXJfcGFydGl0aW9ucyI6IHRydWUsICJhbGxnYXRoZXJfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJvdmVybGFwX2NvbW0iOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAicmVkdWNlX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAiY29udGlndW91c19ncmFkaWVudHMiOiB0cnVlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ==', '--megatron_config', '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'] exits with return code = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Model Into Huggingface Format"
      ],
      "metadata": {
        "id": "OqwSx_Mz5x9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./tools/ckpts/convert_module_to_hf.py --input_dir ./checkpoints/global_step500 --config_file ./configs/125M.yml --output_dir GPT-NeoX-pretrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmiVlDoV0J_q",
        "outputId": "a940e7a7-605c-4afe-e4aa-931c726f934b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "> building HFTokenizer tokenizer ...\n",
            " > padded vocab (size: 30000) with 80 dummy tokens (new size: 30080)\n",
            "Saving weights in fp16 precision...\n",
            "100% 12/12 [00:00<00:00, 74.84it/s]\n",
            "saving tokenizer from file data/tokenizer.json\n",
            "loaded tokenizer:  PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True)\n",
            "tokenizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Pretrained Model"
      ],
      "metadata": {
        "id": "GWhj7Ws86WHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "prompt = \"Hello this is\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt') #.to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, max_length=500)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh6Xp2DUJC8l",
        "outputId": "7a0c66c6-dbb5-433b-9cab-2f1ce7d1b6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Hello this is [SEP] [CLS] \" \" S, \" she's a big person. I'm a problem to her new child's [UNK] is a \" of them \" a statement for the \" video. \" It is the most child that was a child on the victim. We had just his own - profile old on the video : \" We're no one, and's the first time! \" and will get back with the child, which has been a good - time. \" In the day that there was \" in a few - election? \" was so well - with the world, \" will help them a serious idea. \" I am my name with a gun in the show you are going to stop to go away and they did not look on, \" the source said. \" P. said of me, there is an only - way on. \" | | | | | M la [SEP] [CLS] Tweet with its first - half. \" \" We thought, we'll get me that we'd be able to get some things about people and their favorite to help, \" the source said. Some of course, a couple said it would [UNK] ll be the most time it for the same [UNK] but the source [UNK] [UNK] so a few men. \" The woman has been used the \" room in June. \" The latest government. The man is now at the US House last night to make it that they believe the government - like president's mother as it would know a man, where the man is a bad. The police spokesman was set to see the pair of a drug to the victim to make a person on a statement. The man said in a home on a nearby mother and the man [UNK] s body are in the hospital. The woman was called by the shooting were called to the case [UNK] and said it was no details in the world, but the actress and a statement at the past in box on the town. Read or his name, the source reported, when Obama is in the home of an average - threatening - up video, according to that that she was at first point. She took an un written, then I must [UNK] m going to take a different story, [UNK] but a source has just don [UNK] t see, [UNK] said. At a press, a photo is now a way on Friday [UNK] s home. ( I am know to find up to the message out, but he doesn't have seen the country on and then\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload To Huggingface"
      ],
      "metadata": {
        "id": "ilMt1TqA6dUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "\n",
        "tokenizer.push_to_hub(huggingface_path)\n",
        "model.push_to_hub(huggingface_path)"
      ],
      "metadata": {
        "id": "ZEFCjGgE6c3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
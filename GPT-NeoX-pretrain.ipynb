{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download git repository"
      ],
      "metadata": {
        "id": "9C6zDnix06sY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSSCnNL6OCHl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/EleutherAI/gpt-neox.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ],
      "metadata": {
        "id": "IyN0rqGQ1B2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpt-neox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThjL_A20Pb4C",
        "outputId": "a1faa3d1-3ff1-4965-b43a-10c7924dd336"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-neox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements/requirements.txt\n",
        "!pip install tensorboard==2.14\n",
        "!pip install 'urllib3<2'\n",
        "!pip install -r requirements/requirements-wandb.txt # optional, if logging using WandB\n",
        "!pip install -r requirements/requirements-tensorboard.txt # optional, if logging via tensorboard\n",
        "!python ./megatron/fused_kernels/setup.py install # optional, if using fused kernels"
      ],
      "metadata": {
        "id": "reX0I26aPXvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets And Tokenizer\n",
        "Make data folder and put dataset inside the folder.The dataset must be in jsonl format. Else you can make your own dataset into jsonl dataset."
      ],
      "metadata": {
        "id": "aRMNFIl51H_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "U1idnscmPaEI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Custom jsonl Dataset(Optional)"
      ],
      "metadata": {
        "id": "73LhqWHm1UFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "my_dataset = open(\"my_dataset.txt\", 'r').read()\n",
        "my_dataset = my_dataset.split(\"\\n\")\n",
        "\n",
        "# make data into list\n",
        "list_dict = []\n",
        "for i in range(len(my_dataset)):\n",
        "    dic = {}\n",
        "    dic['text'] = my_dataset[i]\n",
        "    list_dict.append(dic)\n",
        "\n",
        "# make list into jsonl format\n",
        "import json\n",
        "with open('my_dataset.jsonl', 'w', encoding='utf-8') as outfile:\n",
        "    for entry in list_dict:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')"
      ],
      "metadata": {
        "id": "hgYBSN2u1Wlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer  \n",
        "There are three options for tokenizer.\n",
        "1. You can download English tokenizers from the link below.\n",
        "2. You can use any tokenizer you want, as long as they are HFGPT2Tokenizer, HFTokenizer, GPT2BPETokenizer, or CharLevelTokenizer.\n",
        "3. You can make custom tokenizer with the code below.   \n",
        "\n",
        "GPT2 Tokenizer\n",
        "- Vocab: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
        "- Merge: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
        "\n",
        "GPT-NeoX 20B Tokenizer  \n",
        "https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/20B_tokenizer.json  \n"
      ],
      "metadata": {
        "id": "aLOCyw-N1rFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \\\n",
        "\t\t\t\t\t\t\t\t\t\t\tvocab_size = 60000, min_frequency = 5)\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "files = [\"./data/tokenizer_data.txt\"]  # has to be txt file\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "awesome_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "awesome_tokenizer.save_pretrained(\"./data\")"
      ],
      "metadata": {
        "id": "5RmO_R94oNBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using custom tokenizer, fix gpt-neox/tools/datasets/preprocess_data.py file as below.  \n",
        "\n",
        "```\n",
        "for key, sentences in doc.items():  # from 223rd line\n",
        "    for sentence in sentences:\n",
        "        sentence = list(filter(None.__ne__, sentence))  # add this line\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cCc4XPc_26LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "Tokenize jsonl file and make into two files, bin and idx."
      ],
      "metadata": {
        "id": "BocmsMJ23xuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/datasets/preprocess_data.py \\\n",
        "            --input /content/gpt-neox/data/my_dataset.jsonl \\  # input file\n",
        "            --output-prefix ./data/mydataset \\  # output file name\n",
        "            --vocab /content/gpt-neox/data/tokenizer.json \\  # tokenizer file\n",
        "            --dataset-impl mmap \\  # dataset implementation, you can choose between lazy, cached, and mmap\n",
        "            --tokenizer-type HFTokenizer \\\n",
        "            --append-eod"
      ],
      "metadata": {
        "id": "KvCXGv2VX4NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "hy13mDtM5h1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent ninja build error"
      ],
      "metadata": {
        "id": "j1PSbdLj4k3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ],
      "metadata": {
        "id": "VqYExi_JZVMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix Configuration Files"
      ],
      "metadata": {
        "id": "zZl8qZyg4xOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix two configuration files, local_setup.yml and your_chosen_parameter_size.yml in gpt-neox/configs folder.  \n",
        "\n",
        "**local_setup.yml**\n",
        "\n",
        "```\n",
        "{\n",
        "  \"data_path\": \"data/mydataset_text_document\",\n",
        "\n",
        "  \"save\": \"checkpoints\",\n",
        "  \"load\": \"checkpoints\",\n",
        "  \"checkpoint_validation_with_forward_pass\": False,\n",
        "\n",
        "  \"tensorboard_dir\": \"tensorboard\",\n",
        "  \"log_dir\": \"logs\",\n",
        "  \"use_wandb\": True,\n",
        "  \"wandb_host\": \"https://api.wandb.ai\",\n",
        "  \"wandb_project\": \"neox\"\n",
        "}\n",
        "```\n",
        "**your_chosen_parameter_size.yml**\n",
        "```\n",
        "// parallelism settings\n",
        "// multiplication of the numbers should be the number of GPUs\n",
        "\t\"pipe_parallel_size\": 1,\n",
        "\t\"model_parallel_size\": 1,\n",
        "\n",
        "// add tokenizer info\n",
        "\"tokenizer_type\": \"HFTokenizer\",\n",
        "\"vocab_file\": \"data/tokenizer.json\",\n",
        "\"merge_file\": \"data/merges.txt\",\n",
        "\n",
        "\n",
        "// choose iteration size you want\n",
        "// train_iters and lr_decay_iters should be the same\n",
        "// checkpoint factor size you want\n",
        "\"train_iters\": 1000,\n",
        "\"lr_decay_iters\": 1000,\n",
        "\"distributed_backend\": \"nccl\",\n",
        "\"lr_decay_style\": \"cosine\",\n",
        "\"warmup\": 0.01,\n",
        "\"checkpoint_factor\": 500,\n",
        "\"eval_interval\": 100000,\n",
        "\"eval_iters\": 10,\n",
        "```\n"
      ],
      "metadata": {
        "id": "bH37njGP5pB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter explaination\n",
        "- **pipe_parallel_size**: number of pipeline parallel stages, has to do with deepspeed\n",
        "pipeline parallel is partitioning the layers of a model into stages that can be processed in parallel, improves memory and compute efficiency\n",
        "- **model_parallel_size**: size of the model parallelism, has to do with Megatron-LM\n",
        "model parallism is splitting model across multiple devices\n",
        "- **tokenizer_type**: choose between ['GPT2BPETokenizer', 'HFTokenizer', 'HFGPT2Tokenizer', 'SPMTokenizer', 'CharLevelTokenizer', 'TiktokenTokenizer']\n",
        "- **merge_file**: only if you have merge.txt file\n",
        "- **train_iters**: number of iterations to run for training\n",
        "- **lr_decay_iters**: number of iterations to decay learning rate over\n",
        "after every n training iterations, learning rate is adjusted\n",
        "default is train_iters\n",
        "- **lr_decay_style**: learning rate decay function, choose between 'constant', 'linear', 'cosine', 'exponential'\n",
        "- **warmup**: percentage of total iterations to warmup on\n",
        "starting with a small learning rate and gradually increasing it during the training\n",
        "- **checkpoint_factor**: choose between ‘log’ or ‘linear\n",
        "    - log: checkpoint will be saved square of the number\n",
        "    - linear: checkpoint will be saved multiplication of the number\n",
        "- **eval_interval**: interval between running evaluation on validation set\n",
        "- **eval_iters**: number of iterations to run for evaluation validation/test for"
      ],
      "metadata": {
        "id": "f-sAU78v5Sk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "aHfD7SnD5sGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./deepy.py train.py -d configs your_chosen_parameter_size.yml local_setup.yml"
      ],
      "metadata": {
        "id": "1iO1MXPrZfzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Model Into Huggingface Format"
      ],
      "metadata": {
        "id": "OqwSx_Mz5x9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./tools/ckpts/convert_module_to_hf.py \\\n",
        "            --input_dir ./checkpoints/global_your_checkpoint \\\n",
        "            --config_file ./configs/your_chosen_parameter_size.yml \\\n",
        "            --output_dir GPT-NeoX-pretrain"
      ],
      "metadata": {
        "id": "UmiVlDoV0J_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Pretrained Model"
      ],
      "metadata": {
        "id": "GWhj7Ws86WHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "prompt = \"Hello this is\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
        "  gen_tokens = model.generate(tokens, do_sample=True, max_length=500)\n",
        "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "bh6Xp2DUJC8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload To Huggingface"
      ],
      "metadata": {
        "id": "ilMt1TqA6dUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"GPT-NeoX-pretrain\")\n",
        "\n",
        "tokenizer.push_to_hub(huggingface_path)\n",
        "model.push_to_hub(huggingface_path)"
      ],
      "metadata": {
        "id": "ZEFCjGgE6c3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
